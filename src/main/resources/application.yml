server:
  port: 8088

spring:
  # 允许同名bean覆盖
  main:
    allow-bean-definition-overriding: true
  datasource:
    # mysql
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://10.207.0.169:3306/sf_mall?useUnicode=true&characterEncoding=utf8&useSSL=false&serverTimezone=Asia/Shanghai&allowMultiQueries=true
    username: root
    password: root
    # druid
    type: com.alibaba.druid.pool.DruidDataSource
    druid:
      initial-size: 5
      min-idle: 5
      max-active: 20
      max-wait: 6000
      time-between-eviction-runs-millis: 60000
      min-evictable-idle-time-millis: 30000
      test-on-borrow: false
      test-on-return: false
      pool-prepared-statements: true
      max-pool-prepared-statement-per-connection-size: 20
    # 初始化batch表(方式一)
    #schema: classpath:org/springframework/batch/core/schema-mysql.sql
    #initialization-mode: always
  # batch
  batch:
    # 初始化batch表(方式二)
    initialize-schema: always
    # 读取batch表加的表前缀
    #table-prefix: ZERO_BATCH_
    job:
      # 禁止项目启动时运行job，默认true
      enabled: true
      # 启动时要执⾏的Job，不配置则执行所有
      names: jobDemo
  # thymeleaf
  thymeleaf:
    prefix: classpath:/templates/
    check-template-location: true
    suffix: .html
    encoding: UTF-8
    mode: HTML
    cache: false  # false页面热部署会生效
    servlet:
      content-type: text/html
  # devtools
  devtools:
    restart:
      enabled: true  #设置开启热部署
      additional-paths: src/main/java #重启目录
      exclude: static/**,WEB-INF/**
  # flyway
  flyway:
    # 是否启用flyway
    enabled: true
    # 编码格式，默认UTF-8
    encoding: UTF-8
    # 修改默认表名(flyway_schema_history)，可实现多个模块分别通过Flyway进行数据库版本控制
    table: flyway_schema_history_zero
    # 迁移sql脚本文件存放路径，默认db/migration
    locations: classpath:db/migration
    # 迁移sql脚本文件名称的前缀，默认V
    sql-migration-prefix: V
    # 迁移sql脚本文件名称的分隔符，默认2个下划线__
    sql-migration-separator: __
    # 迁移sql脚本文件名称的后缀
    sql-migration-suffixes: .sql
    # 迁移时是否进行校验，默认true
    validate-on-migrate: true
    # 对执行迁移时基准版本的描述
    baseline-description: test
    # 当迁移时发现目标schema非空，而且带有没有元数据的表时，是否自动执行基准迁移，默认false.
    # 设置为 true 后 flyway 将在需要 baseline 的时候, 自动执行一次 baseline。
    baseline-on-migrate: true
    # 指定 baseline 的版本号,默认值为 1, 低于该版本号的 SQL 文件, migrate 时会被忽略
    baseline-version: 1
    # 是否禁用清库，生产务必设置成 true
    clean-disabled: true
    # 是否允许不按顺序迁移 开发建议 true  生产建议 false
    out-of-order: true
    # 检查迁移脚本的位置是否存在，默认false
    check-location: false
    # 当读取元数据表时是否忽略错误的迁移，默认false
    ignore-future-migrations: false
    # 当初始化好连接时要执行的SQL
    init-sqls: show tables;
  # kafka
  kafka:
    # 集群信息，多个用逗号隔开
    bootstrap-servers: 10.207.0.167:9092
    #【生产者】
    producer:
      # 事务ID=事务前缀+1，不为空即开启事务，生效必须 retries>0，acks=all
      transaction-id-prefix: kfk_tx_
      # 重试次数
      retries: 1
      # 应答级别:多少个分区副本备份完成时向生产者发送ack确认(可选0、1、all/-1)
      #procedure要求leader在考虑完成请求之前收到的确认数，用于控制发送记录在服务端的持久化，其值可以为如下：
      #acks = 0 如果设置为零，则生产者将不会等待来自服务器的任何确认，该记录将立即添加到套接字缓冲区并视为已发送。在这种情况下，无法保证服务器已收到记录，并且重试配置将不会生效（因为客户端通常不会知道任何故障），为每条记录返回的偏移量始终设置为-1。
      #acks = 1 这意味着leader会将记录写入其本地日志，但无需等待所有副本服务器的完全确认即可做出回应，在这种情况下，如果leader在确认记录后立即失败，但在将数据复制到所有的副本服务器之前，则记录将会丢失。
      #acks = all 这意味着leader将等待完整的同步副本集以确认记录，这保证了只要至少一个同步副本服务器仍然存活，记录就不会丢失，这是最强有力的保证，这相当于acks = -1的设置。
      acks: all
      # 每次批量发送消息的数量，16K
      batch-size: 16384
      # linger.ms为0表示每接收到一条消息就提交给kafka,这时候batch-size其实就没用了
      properties:
        linger.ms: 0
        # 自定义分区器，实现Partitioner接口
        # partitioner:
          # class: com.zeor.producer.CustomizePartitioner
      # 生产端缓冲区大小，32M
      buffer-memory: 33554432
      # 指定消息key和消息体的编解码方式，也可自定义
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: com.sf.kafka.serialization.ObjectSerializer
    #【生产者】
    consumer:
      # 指定默认消费者group id
      group-id: zeroGroupId
      # 是否自动提交offset
      enable-auto-commit: false
      properties:
        # 消费会话超时时间(超过这个时间consumer没有发送心跳,就会触发rebalance操作)
        session.timeout.ms: 120000
        # 消费请求超时时间
        request.timeout.ms: 180000
      # 默认为500，批量消费每次最多消费多少条消息
      #max-poll-records: 50
      # 提交offset延时(接收到消息后多久提交offset)
      auto-commit-interval: 100
      # 当kafka中没有初始offset或offset超出范围时将自动重置offset
      # earliest:当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费，避免消息丢失
      # latest:重置为分区中最新的offset(消费分区中新产生的数据);
      # none:只要有一个分区不存在已提交的offset,就抛出异常;
      auto-offset-reset: earliest
      # 指定消息key和消息体的编解码方式
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: com.sf.kafka.serialization.ObjectDeserializer
    listener:
      # 指定listener容器中的线程数（同时消费的监听器），用于提高并发量，建议与分区数量⼀致
      concurrency: 3
      # 消费端监听的topic不存在时，项目启动会报错(关掉)
      missing-topics-fatal: false
      # 设置批量消费
      #type: batch
      # ACK模式：batch、record、time、count、count_time、manual、manual_immediate
      # 默认batch，手动调用Acknowledgment.acknowledge()后立即提交，一般使用这种
      ack-mode: manual_immediate

# mybatis-plus
mybatis-plus:
  mapper-locations: classpath:mapper/*.xml
  typeAliasesPackage: com.sf.bean
  configuration:
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl
    map-underscore-to-camel-case: true

# 自定义配置
kafka:
  topic:
    group-id: zeroGroupId
    topic-name:
      - zero
      - zero1
      - sun